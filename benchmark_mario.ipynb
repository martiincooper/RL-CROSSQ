{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mario Bros RL Benchmark: CrossQ vs DroQ vs RedQ\n",
                "\n",
                "This notebook benchmarks three advanced specific RL algorithms (implemented via `sbx`'s SAC with modifications) on `SuperMarioBros-v0`.\n",
                "It is self-contained (requires `sbx` package from this repo) and includes all necessary wrappers."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Colab Setup\n",
                "**Instructions:**\n",
                "1. **Git Push**: Ensure you have pushed your local changes (with `sbx` updates) to your GitHub fork.\n",
                "2. **Update URL**: Replace `YOUR_GITHUB_REPO_URL` below with the URL of your forked repository.\n",
                "3. **Run**: Execute the cell to install dependencies and the local package."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Install Dependencies\n",
                "import os\n",
                "\n",
                "# ==========================================\n",
                "# CHANGE THIS TO YOUR FORK URL\n",
                "YOUR_GITHUB_REPO_URL = \"https://github.com/martiincooper/RL-CROSSQ.git\"\n",
                "# ==========================================\n",
                "\n",
                "repo_name = YOUR_GITHUB_REPO_URL.split(\"/\")[-1].replace(\".git\", \"\")\n",
                "\n",
                "# 1. Clone if setup.py is missing and we aren't in the repo folder yet\n",
                "if not os.path.exists(\"setup.py\"):\n",
                "    if not os.path.exists(repo_name):\n",
                "        print(f\"Cloning {YOUR_GITHUB_REPO_URL}...\")\n",
                "        !git clone $YOUR_GITHUB_REPO_URL\n",
                "    \n",
                "    # Configure Python to work inside the repo\n",
                "    if os.path.exists(repo_name):\n",
                "        os.chdir(repo_name)\n",
                "        print(f\"Changed directory to {os.getcwd()}\")\n",
                "\n",
                "# 2. Install project in editable mode so 'sbx' is importable\n",
                "# We use --ignore-requires-python to bypass strict python checks\n",
                "# WE USE ABSOLUTE PATH to ensure pip finds it regardless of shell CWD\n",
                "cwd = os.getcwd()\n",
                "!pip install -e \"$cwd\" --ignore-requires-python\n",
                "\n",
                "# 3. Install compatible versions of libraries\n",
                "# gymnasium==0.29.1 is strict requirement for wrappers\n",
                "# numpy<2.0.0 is required for scipy and legacy gym compatibility\n",
                "# flax>=0.8.0 is required for compatibility with newer JAX versions\n",
                "!pip install \"numpy<2.0.0\" \"flax>=0.8.0\" gymnasium==0.29.1 gym-super-mario-bros==7.4.0 nes-py==8.2.1 shimmy==1.3.0 opencv-python matplotlib stable-baselines3 tensorflow-probability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Ensure we can import sbx if we are in the cloned repo\n",
                "if os.getcwd() not in sys.path:\n",
                "    sys.path.append(os.getcwd())\n",
                "\n",
                "try:\n",
                "    import gymnasium as gym\n",
                "    from gymnasium.wrappers import ResizeObservation, GrayScaleObservation, FrameStack\n",
                "except ImportError as e:\n",
                "    print(\"ImportError:\", e)\n",
                "    print(\"Please install gymnasium==0.29.1: pip install gymnasium==0.29.1\")\n",
                "\n",
                "import gym_super_mario_bros\n",
                "from nes_py.wrappers import JoypadSpace\n",
                "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
                "import numpy as np\n",
                "import cv2\n",
                "import jax\n",
                "import optax\n",
                "import tensorflow_probability as tfp\n",
                "\n",
                "# Verify TFP JAX backend\n",
                "try:\n",
                "    tfd = tfp.substrates.jax.distributions\n",
                "    print(\"TFP JAX backend functional.\")\n",
                "except AttributeError:\n",
                "    print(\"Error: tensorflow_probability.substrates.jax not found. Please upgrade tensorflow-probability.\")\n",
                "\n",
                "try:\n",
                "    from sbx import SAC\n",
                "    from sbx.sac.utils import ReLU\n",
                "except ImportError:\n",
                "    print(\"Could not import sbx. Make sure you are running this notebook from the CrossQ repository root or have installed it via 'pip install -e .'\")\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from stable_baselines3.common.monitor import Monitor\n",
                "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
                "\n",
                "# Ensure compatibility\n",
                "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
                "\n",
                "ENV_ID = 'SuperMarioBros-v0'\n",
                "TOTAL_TIMESTEPS = 100000 # Adjust as needed\n",
                "LOG_DIR = \"./mario_benchmark_logs/\"\n",
                "os.makedirs(LOG_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"JAX Devices: {jax.devices()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Wrappers\n",
                "We define the environment wrappers here to make the notebook self-contained."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Robust wrapper to force Gymnasium API (5-tuple step)\n",
                "class MarioGymnasiumWrapper(gym.Wrapper):\n",
                "    def __init__(self, env):\n",
                "        # We manually wrap the gym environment\n",
                "        self.env = env\n",
                "        # Do not copy attributes manually, let Wrapper delegate or Properties handle it.\n",
                "        \n",
                "        # We try to get them from the unwrapped env if possible\n",
                "        # And convert to gymnasium spaces\n",
                "        gym_obs = env.observation_space\n",
                "        self._observation_space = gym.spaces.Box(\n",
                "            low=gym_obs.low, high=gym_obs.high, shape=gym_obs.shape, dtype=gym_obs.dtype\n",
                "        )\n",
                "        \n",
                "        gym_act = env.action_space\n",
                "        if hasattr(gym_act, 'n'):\n",
                "            self._action_space = gym.spaces.Discrete(gym_act.n)\n",
                "        else:\n",
                "             self._action_space = gym.spaces.Box(\n",
                "                low=gym_act.low, high=gym_act.high, shape=gym_act.shape, dtype=gym_act.dtype\n",
                "             )\n",
                "\n",
                "        self._metadata = getattr(env, 'metadata', {})\n",
                "        self._reward_range = getattr(env, 'reward_range', (-float('inf'), float('inf')))\n",
                "\n",
                "    @property\n",
                "    def action_space(self):\n",
                "        return self._action_space\n",
                "\n",
                "    @property\n",
                "    def observation_space(self):\n",
                "        return self._observation_space\n",
                "\n",
                "    @property\n",
                "    def metadata(self):\n",
                "        return self._metadata\n",
                "\n",
                "    @property\n",
                "    def reward_range(self):\n",
                "        return self._reward_range\n",
                "\n",
                "    def reset(self, **kwargs):\n",
                "        # gym-super-mario-bros reset returns (obs) or (obs, info) depending on version/wrapper\n",
                "        # Handle seed manually for old gym compatibility\n",
                "        seed = kwargs.get('seed', None)\n",
                "        options = kwargs.get('options', None)\n",
                "        \n",
                "        # Pop them to avoid passing to old reset which might not accept them\n",
                "        if 'seed' in kwargs: kwargs.pop('seed')\n",
                "        if 'options' in kwargs: kwargs.pop('options')\n",
                "        \n",
                "        if seed is not None:\n",
                "             try:\n",
                "                 self.env.seed(seed)\n",
                "             except AttributeError:\n",
                "                 pass # Env might not support seed\n",
                "        \n",
                "        # usage: obs = env.reset()\n",
                "        ret = self.env.reset(**kwargs)\n",
                "        if isinstance(ret, tuple):\n",
                "            if len(ret) == 2:\n",
                "                return ret\n",
                "            return ret[0], {}\n",
                "        return ret, {}\n",
                "\n",
                "    def step(self, action):\n",
                "        ret = self.env.step(action)\n",
                "        if len(ret) == 4:\n",
                "            obs, reward, done, info = ret\n",
                "            truncated = False\n",
                "            terminated = done\n",
                "            return obs, reward, terminated, truncated, info\n",
                "        elif len(ret) == 5:\n",
                "            return ret\n",
                "        else:\n",
                "            raise ValueError(f\"Unexpected step return length: {len(ret)}\")\n",
                "    \n",
                "    def render(self):\n",
                "        return self.env.render()\n",
                "    \n",
                "    def close(self):\n",
                "        return self.env.close()\n",
                "\n",
                "class ContinuousMarioWrapper(gym.ActionWrapper):\n",
                "    def __init__(self, env):\n",
                "        super().__init__(env)\n",
                "        n_actions = env.action_space.n\n",
                "        # CrossQ/SAC expect a Box action space\n",
                "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(n_actions,), dtype=np.float32)\n",
                "        \n",
                "    def action(self, action):\n",
                "        # Convert continuous vector to discrete index\n",
                "        return int(np.argmax(action))\n",
                "\n",
                "# Transpose to (H, W, C) = (84, 84, 4)\n",
                "class TransposeWrapper(gym.ObservationWrapper):\n",
                "    def __init__(self, env):\n",
                "        super().__init__(env)\n",
                "        old_shape = env.observation_space.shape # (4, 84, 84)\n",
                "        self.observation_space = gym.spaces.Box(\n",
                "            low=0, high=255, \n",
                "            shape=(old_shape[1], old_shape[2], old_shape[0]), \n",
                "            dtype=env.observation_space.dtype\n",
                "        )\n",
                "    \n",
                "    def observation(self, obs):\n",
                "        # obs is (4, 84, 84) -> (84, 84, 4)\n",
                "        return np.moveaxis(obs, 0, -1)\n",
                "\n",
                "def make_mario_env(env_id='SuperMarioBros-v0', action_space=SIMPLE_MOVEMENT, stack_frames=4, render_mode='rgb_array'):\n",
                "    # Create the original environment\n",
                "    # nes-py returns 4-tuple, gym 0.26 expects 5-tuple. compatibility=True might help.\n",
                "    env = gym_super_mario_bros.make(env_id, apply_api_compatibility=True) \n",
                "    env = JoypadSpace(env, action_space)\n",
                "    \n",
                "    # Force load (nes_py lazy initialization might be an issue)\n",
                "    env.reset()\n",
                "    \n",
                "    # Wrap it to gymnasium manually\n",
                "    env = MarioGymnasiumWrapper(env)\n",
                "\n",
                "    # Convert to continuous for CrossQ/SAC\n",
                "    env = ContinuousMarioWrapper(env)\n",
                "    \n",
                "    # Apply wrappers\n",
                "    env = GrayScaleObservation(env, keep_dim=False)\n",
                "    env = ResizeObservation(env, (84, 84))\n",
                "    env = FrameStack(env, stack_frames)\n",
                "    \n",
                "    env = TransposeWrapper(env)\n",
                "    \n",
                "    return env"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Algorithm Configuration\n",
                "We define helper functions to create the specific configurations for CrossQ, DroQ, and RedQ based on the paper implementations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_model(algo_name, env, seed=1):\n",
                "    # Common defaults\n",
                "    net_arch = {\"pi\": [256, 256], \"qf\": [256, 256]}\n",
                "    kwargs = {\n",
                "        \"verbose\": 1,\n",
                "        \"seed\": seed,\n",
                "        \"learning_starts\": 1000,\n",
                "        \"buffer_size\": 50_000, # Reduced to avoid OOM on laptop\n",
                "        \"ent_coef\": \"auto\",\n",
                "        \"policy_kwargs\": {\n",
                "             \"activation_fn\": ReLU,\n",
                "        },\n",
                "    }\n",
                "    \n",
                "    extra_policy_kwargs = {}\n",
                "    \n",
                "    if algo_name.lower() == \"crossq\":\n",
                "        # CrossQ Specifics\n",
                "        kwargs.update({\n",
                "            \"crossq_style\": True,\n",
                "            \"policy_delay\": 3,\n",
                "            \"gradient_steps\": 1, # UTD=1\n",
                "            \"tau\": 1.0,          # No target network\n",
                "            \"learning_rate\": 1e-3,\n",
                "        })\n",
                "        extra_policy_kwargs = {\n",
                "            \"n_critics\": 2,\n",
                "            \"batch_norm\": True,\n",
                "            \"batch_norm_momentum\": 0.99,\n",
                "            \"net_arch\": {\"pi\": [256, 256], \"qf\": [2048, 2048]}, # Wider critics for CrossQ\n",
                "            \"optimizer_kwargs\": {\"b1\": 0.5}\n",
                "        }\n",
                "        \n",
                "    elif algo_name.lower() == \"droq\":\n",
                "        # DroQ Specifics\n",
                "        kwargs.update({\n",
                "            \"gradient_steps\": 20, # UTD=20\n",
                "            \"policy_delay\": 20,\n",
                "            \"learning_rate\": 3e-4,\n",
                "        })\n",
                "        extra_policy_kwargs = {\n",
                "            \"n_critics\": 2,\n",
                "            \"dropout_rate\": 0.01,\n",
                "            \"layer_norm\": True,\n",
                "            \"net_arch\": net_arch,\n",
                "        }\n",
                "        \n",
                "    elif algo_name.lower() == \"redq\":\n",
                "        # RedQ Specifics\n",
                "        kwargs.update({\n",
                "            \"gradient_steps\": 20, # UTD=20\n",
                "            \"policy_delay\": 20,\n",
                "             \"learning_rate\": 3e-4,\n",
                "        })\n",
                "        extra_policy_kwargs = {\n",
                "            \"n_critics\": 10,\n",
                "            \"net_arch\": net_arch,\n",
                "        }\n",
                "        \n",
                "    # Combine kwargs\n",
                "    kwargs[\"policy_kwargs\"].update(extra_policy_kwargs)\n",
                "\n",
                "    model = SAC(\n",
                "        \"CnnPolicy\", \n",
                "        env,\n",
                "        **kwargs\n",
                "    )\n",
                "    model.name = algo_name\n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop\n",
                "We train each model sequentially. For demonstration, we train for a limited number of steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create environments with Monitor\n",
                "env_crossq = Monitor(make_mario_env(ENV_ID), filename=os.path.join(LOG_DIR, \"CrossQ\"))\n",
                "env_droq = Monitor(make_mario_env(ENV_ID), filename=os.path.join(LOG_DIR, \"DroQ\"))\n",
                "env_redq = Monitor(make_mario_env(ENV_ID), filename=os.path.join(LOG_DIR, \"RedQ\"))\n",
                "\n",
                "# Initialize models\n",
                "model_crossq = create_model(\"CrossQ\", env_crossq)\n",
                "model_droq = create_model(\"DroQ\", env_droq)\n",
                "model_redq = create_model(\"RedQ\", env_redq)\n",
                "\n",
                "models = [model_crossq, model_droq, model_redq]\n",
                "\n",
                "print(\"Models initialized. Starting training...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for model in models:\n",
                "    print(f\"Training {model.name} for {TOTAL_TIMESTEPS} steps...\")\n",
                "    model.learn(total_timesteps=TOTAL_TIMESTEPS, progress_bar=True)\n",
                "    model.save(f\"sbx_{model.name}_mario\")\n",
                "    print(f\"{model.name} Done.\")\n",
                "    model.env.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Performance Comparison\n",
                "Plotting the learning curves (smoothed episode rewards)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def moving_average(values, window):\n",
                "    \"\"\"Smooth values by calculating moving average.\"\"\"\n",
                "    if len(values) < window:\n",
                "        return values\n",
                "    weights = np.repeat(1.0, window) / window\n",
                "    return np.convolve(values, weights, 'valid')\n",
                "\n",
                "def plot_results(log_folder, title='Learning Curves'):\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    \n",
                "    algos = [\"CrossQ\", \"DroQ\", \"RedQ\"]\n",
                "    colors = ['r', 'g', 'b']\n",
                "    \n",
                "    for algo, color in zip(algos, colors):\n",
                "        try:\n",
                "            # Load specific monitor file\n",
                "            # Monitor files are named {algo}.monitor.csv\n",
                "            if not os.path.exists(os.path.join(log_folder, f\"{algo}.monitor.csv\")):\n",
                "                print(f\"Log for {algo} not found, skipping.\")\n",
                "                continue\n",
                "            \n",
                "            import pandas as pd\n",
                "            df = pd.read_csv(os.path.join(log_folder, f\"{algo}.monitor.csv\"), skiprows=1)\n",
                "            x = df['t'].values\n",
                "            y = df['r'].values\n",
                "            \n",
                "            if len(x) > 1:\n",
                "                y_smoothed = moving_average(y, window=50)\n",
                "                x_smoothed = x[len(x) - len(y_smoothed):]\n",
                "                plt.plot(x_smoothed, y_smoothed, label=algo, color=color)\n",
                "            else:\n",
                "                print(f\"Not enough data to plot for {algo}\")\n",
                "        except Exception as e:\n",
                "            print(f\"Could not plot {algo}: {e}\")\n",
                "            \n",
                "    plt.xlabel('Timesteps')\n",
                "    plt.ylabel('Episode Reward')\n",
                "    plt.title(title)\n",
                "    plt.legend()\n",
                "    plt.grid(True)\n",
                "    plt.show()\n",
                "\n",
                "plot_results(LOG_DIR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visual Benchmark (The Race)\n",
                "Now we watch them play side-by-side.\n",
                "**Note:** `cv2.imshow` does not work in Google Colab. If running on Colab, we can use `google.colab.patches.cv2_imshow` or generate a video."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_race(models, env_id):\n",
                "    # Create fresh evaluation envs\n",
                "    envs = [make_mario_env(env_id) for _ in models]\n",
                "    obs_list = [env.reset()[0] for env in envs]\n",
                "    dones = [False] * len(models)\n",
                "    \n",
                "    print(\"Starting Race! Check the popup window (or output below if on Colab).\")\n",
                "    \n",
                "    try:\n",
                "        from google.colab.patches import cv2_imshow\n",
                "        is_colab = True\n",
                "    except ImportError:\n",
                "        is_colab = False\n",
                "\n",
                "    try:\n",
                "        # Run for a fixed number of frames to avoid infinite loops in notebook\n",
                "        for _ in range(500):\n",
                "            if all(dones): break\n",
                "\n",
                "            frames = []\n",
                "            for i, (model, env) in enumerate(zip(models, envs)):\n",
                "                if not dones[i]:\n",
                "                    action, _ = model.predict(obs_list[i], deterministic=True)\n",
                "                    obs, reward, terminated, truncated, info = env.step(action)\n",
                "                    obs_list[i] = obs\n",
                "                    \n",
                "                    done = terminated or truncated\n",
                "                    dones[i] = done\n",
                "                    \n",
                "                    frame = env.render() # Returns rgb array\n",
                "                    frame = np.ascontiguousarray(frame, dtype=np.uint8)\n",
                "                    cv2.putText(frame, model.name, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
                "                    frames.append(frame)\n",
                "                else:\n",
                "                    # Show last frame or black\n",
                "                    blank = np.zeros((240, 256, 3), dtype=np.uint8)\n",
                "                    cv2.putText(blank, f\"{model.name} Finished\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
                "                    frames.append(blank)\n",
                "            \n",
                "            # Concat frames horizontally\n",
                "            combined = np.hstack(frames)\n",
                "            # Convert RGB to BGR for OpenCV\n",
                "            combined_bgr = cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)\n",
                "            \n",
                "            if is_colab:\n",
                "                # Clear previous output for animation effect (optional, or just show last frame)\n",
                "                # cv2_imshow(combined_bgr) \n",
                "                # print(\"\\r\", end=\"\") \n",
                "                pass # Animation in Colab is tricky with cv2_imshow, better to save video\n",
                "            else:\n",
                "                cv2.imshow(\"Mario Race: CrossQ vs DroQ vs RedQ\", combined_bgr)\n",
                "                if cv2.waitKey(20) & 0xFF == ord('q'):\n",
                "                    break\n",
                "    finally:\n",
                "        for env in envs: env.close()\n",
                "        if not is_colab:\n",
                "            cv2.destroyAllWindows()\n",
                "            cv2.waitKey(1)\n",
                "\n",
                "# Load models if needed\n",
                "# ...\n",
                "\n",
                "run_race(models, ENV_ID)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}